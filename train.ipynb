{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42dd27d3-b9f2-4b30-ab5d-5bf6b910af51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/sush/miniconda3/envs/llm/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA exception! Error code: no CUDA-capable device is detected\n",
      "CUDA exception! Error code: initialization error\n",
      "CUDA SETUP: CUDA runtime path found: /home/sush/miniconda3/envs/llm/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/sush/miniconda3/envs/llm/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sush/miniconda3/envs/llm/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No GPU detected! Check your CUDA paths. Proceeding to load CPU-only library...\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "\"\"\"\n",
    "Unused imports:\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\"\"\"\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    PreTrainedTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce07bd0c-ffe5-40d3-85e1-56d9314af2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path: str = \"yahma/alpaca-cleaned\"\n",
    "output_dir: str = \"./output/v1\"\n",
    "# training hyperparams\n",
    "batch_size: int = 128\n",
    "micro_batch_size: int = 1\n",
    "num_epochs: int = 3\n",
    "learning_rate: float = 3e-4\n",
    "# cutoff_len: int = 256\n",
    "val_set_size: int = 2000\n",
    "# lora hyperparams\n",
    "lora_r: int = 8\n",
    "lora_alpha: int = 16\n",
    "lora_dropout: float = 0.05\n",
    "lora_target_modules = \"query_key_value\",\n",
    "# llm hyperparams\n",
    "train_on_inputs: bool = True # if False, masks out inputs in loss\n",
    "add_eos_token: bool = True\n",
    "group_by_length: bool = False  # faster, but produces an odd training loss curve\n",
    "# wandb params\n",
    "wandb_project: str = \"\"\n",
    "wandb_run_name: str = \"\"\n",
    "wandb_watch: str = \"\"  # options: false | gradients | all\n",
    "wandb_log_model: str = \"\"  # options: false | true\n",
    "resume_from_checkpoint: str = None  # either training checkpoint or final adapter\n",
    "# prompt_template_name: str = \"alpaca\"\n",
    "test_size = 0.1\n",
    "seed = 42\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c7d1eb-42f7-46d0-bfc9-25824fc5520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map = \"auto\"\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "ddp = world_size != 1\n",
    "if ddp:\n",
    "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps // world_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59722486-d11d-4474-9866-2ecd400c20cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_tokenizer(pretrained_model_name_or_path) -> PreTrainedTokenizer:\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "#     #tokenizer.add_special_tokens({\"additional_special_tokens\": [END_KEY, INSTRUCTION_KEY, RESPONSE_KEY_NL]})\n",
    "#     return tokenizer\n",
    "\n",
    "\n",
    "# def load_model(\n",
    "#     pretrained_model_name_or_path: str, *, gradient_checkpointing: bool = False\n",
    "# ) -> AutoModelForCausalLM:\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         pretrained_model_name_or_path, \n",
    "#         trust_remote_code=True, \n",
    "#         load_in_8bit=True,\n",
    "#         torch_dtype=torch.bfloat16,\n",
    "#         device_map='auto',\n",
    "#         use_cache=False if gradient_checkpointing else True\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def get_model_tokenizer(\n",
    "#     pretrained_model_name_or_path: str, *, gradient_checkpointing: bool = False\n",
    "# ) -> Tuple[AutoModelForCausalLM, PreTrainedTokenizer]:\n",
    "#     tokenizer = load_tokenizer(pretrained_model_name_or_path)\n",
    "#     model = load_model(pretrained_model_name_or_path, gradient_checkpointing=gradient_checkpointing)\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#     return model, tokenizer\n",
    "# model, tokenizer = get_model_tokenizer(\"databricks/dolly-v2-3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e9f1b5-61dc-429e-8348-3e644d0d39aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast\n",
    "\n",
    "base_model = \"databricks/dolly-v2-3b\"\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        # load_in_8bit=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained(base_model)\n",
    "\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afef70e7-d42e-4b7f-89b9-40ee0c17a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_from_disk(\"data/polyglot_processed_2000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01e87ba8-6d2b-4987-b8f8-2d2716474074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RESPONSE_KEY_NL = \"ENTITIES: \"\n",
    "\n",
    "class DataCollatorForCompletionOnlyLM(DataCollatorForLanguageModeling):\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        # The prompt ends with the response key plus a newline.  We encode this and then try to find it in the\n",
    "        # sequence of tokens.  This should just be a single token.\n",
    "        response_token_ids = self.tokenizer.encode(RESPONSE_KEY_NL)\n",
    "\n",
    "        labels = batch[\"labels\"].clone()\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "\n",
    "            response_token_ids_start_idx = None\n",
    "            for idx in np.where(batch[\"labels\"][i] == response_token_ids[0])[0]:\n",
    "                response_token_ids_start_idx = idx\n",
    "                break\n",
    "\n",
    "            if response_token_ids_start_idx is None:\n",
    "                raise RuntimeError(\n",
    "                    f'Could not find response key {response_token_ids} in token IDs {batch[\"labels\"][i]}'\n",
    "                )\n",
    "\n",
    "            response_token_ids_end_idx = response_token_ids_start_idx + 1\n",
    "\n",
    "            # Make pytorch loss function ignore all tokens up through the end of the response key\n",
    "            labels[i, :response_token_ids_end_idx] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a29902d-1a5f-45c3-9898-8a1dfa76b440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataCollator = DataCollatorForLanguageModeling(\n",
    "    tokenizer = tokenizer,\n",
    "    mlm = False,\n",
    "    pad_to_multiple_of = 8,\n",
    "    return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbb5e6ff-c18f-43a5-b195-8064521307f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sush/miniconda3/envs/llm/lib/python3.9/site-packages/peft/utils/other.py:76: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "816455eb-0c14-4c25-a589-5b2bc1c94079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /mnt/e/Projects/generic_ner/data/polyglot_processed_2000/cache-258eb9386a72366b.arrow and /mnt/e/Projects/generic_ner/data/polyglot_processed_2000/cache-805704d532b2f14d.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = dataset.train_test_split(test_size=test_size, seed=seed)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bc96639-78d4-4ae3-8f51-e8c687291721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test'],\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=1,\n",
    "            warmup_steps=100,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            bf16=True,\n",
    "            logging_steps=1,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=200 if val_set_size > 0 else None,\n",
    "            save_steps=200,\n",
    "            output_dir=output_dir,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "            ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=group_by_length,\n",
    "            report_to=None,\n",
    "            # optim=\"adamw_bnb_8bit\",\n",
    "            # run_name=wandb_run_name if use_wandb else None,\n",
    "        ),\n",
    "        data_collator = DataCollatorForCompletionOnlyLM(\n",
    "            tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9e7817a-ea69-485c-b2d6-e9539d39466e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1800\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5400\n",
      "  Number of trainable parameters = 2621440\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5400' max='5400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5400/5400 19:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.469492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.307800</td>\n",
       "      <td>0.409857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.312200</td>\n",
       "      <td>0.262220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.242800</td>\n",
       "      <td>0.252540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.150600</td>\n",
       "      <td>0.298132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.532700</td>\n",
       "      <td>0.248937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.223334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>0.248102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.202908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.417900</td>\n",
       "      <td>0.221266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.217005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.203290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.213820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.243876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.211078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.371500</td>\n",
       "      <td>0.219235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.234658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.235936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-200\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-400\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-600\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-800\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-1000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-1200\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-1400\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-1600\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-1800\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-2000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-2200\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-2400\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-2600\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-2200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-2800\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-2400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-3000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-2600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-3200\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-2800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-3400\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-3600\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-3200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-3800\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-3400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-4000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-3600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-4200\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-3800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-4400\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-4600\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-4200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-4800\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-4400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-5000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-4600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-5200\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-4800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./lora-alpaca/checkpoint-5400\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [lora-alpaca/checkpoint-5000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./lora-alpaca/checkpoint-1800 (score: 0.2029079645872116).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5400, training_loss=0.10001079444363886, metrics={'train_runtime': 1186.4046, 'train_samples_per_second': 4.552, 'train_steps_per_second': 4.552, 'total_flos': 9707639735992320.0, 'train_loss': 0.10001079444363886, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425c0c0-a971-4c73-b5b9-9b1a0601dd07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db825d-6fef-45b0-b674-91d8577d7c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2f7a2fbc-a3b7-4916-8112-1beea73a668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '''Your job is to extract the entities provided the definition of each entity which is provided below.\n",
    "\n",
    "ORG: Represents a formal group or entity such as a company or organization.\n",
    "PER: Refers to an individual person or a group of individuals.\n",
    "LOC: Represents a specific place or geographical location.\n",
    "\n",
    "QUERY: \"Why are you working?\"\n",
    "ENTITIES:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5e3e5d49-7ccb-4002-b47b-e9924e6a5f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(txt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f3a2d086-bb15-4830-a4a2-041742bed2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = trainer.model.generate(input_ids=input_ids, \n",
    "                                       max_length=200, \n",
    "                                       # num_beams=5,\n",
    "                                       do_sample=True,\n",
    "                                       top_k=2,\n",
    "                                       # no_repeat_ngram_size=2,\n",
    "                                       temperature=0.00001,\n",
    "                                       early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "33b4f1c4-f16a-4a69-b2e7-aedaed5ad5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your job is to extract the entities provided the definition of each entity which is provided below.\n",
      "\n",
      "ORG: Represents a formal group or entity such as a company or organization.\n",
      "PER: Refers to an individual person or a group of individuals.\n",
      "LOC: Represents a specific place or geographical location.\n",
      "\n",
      "QUERY: \"Why are you working?\"\n",
      "ENTITIES:\n",
      "PER: \"John\", \"Doe\"\n",
      "LOC: \"London\"\n",
      "ORG: \"John\", \"Doe\"\n",
      "PER: \"John\", \"Doe\"\n",
      "LOC: \"London\"\n",
      "ORG: \"John\", \"Doe\"\n",
      "PER: \"John\", \"Doe\"\n",
      "LOC: \"London\"\n",
      "ORG: \"John\", \"Doe\"\n",
      "PER: \"John\", \"Doe\"\n",
      "LOC: \"London\"\n",
      "ORG: \"John\", \"Doe\"\n",
      "PER: \"John\", \"Doe\"\n",
      "LOC\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dde97a4b-1e1c-4e4c-9f50-f1669f15b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained('models/v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e655e2e-59ec-4c5a-8a34-5eb99aa90439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
